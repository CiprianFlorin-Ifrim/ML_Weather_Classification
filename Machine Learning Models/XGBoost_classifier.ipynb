{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0969d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------LIBRARIES--------------------------------------------------------------------------------------------                                                                                         #import OpenCV2 library for image processing and algorithms\n",
    "import math\n",
    "import csv \n",
    "import numpy as np                                                                                     #import numpy mathematical library\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt      #import matplotlib library for plotting\n",
    "from micromlgen import port\n",
    "\n",
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))                                  #change width of Jupyer Notebook to use the whole window resolution availab\n",
    "\n",
    "# import the classifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_statistics(clf, X_test, y_test, csv_filename, confusion_matrix_filename, metrics_filename):\n",
    "    y_pred = []\n",
    "    with open(csv_filename, \"a\", newline='') as fp:\n",
    "        for row in X_test:\n",
    "            result = int((str(clf.predict(row.reshape(1, -1))).replace('[','').replace(']','')))\n",
    "            y_pred.append(result)\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            wr.writerow([result])\n",
    "    fp.close()\n",
    "    \n",
    "    #https://www.baeldung.com/cs/multi-class-f1-score\n",
    "    #https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case\n",
    "    precision, recall, fscore, support = score(y_test.ravel(), y_pred)\n",
    "    \n",
    "    #https://towardsdatascience.com/understanding-the-confusion-matrix-from-scikit-learn-c51d88929c79\n",
    "    #https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "    metrics = {\"Accuracy: \" : clf.score(X_test, y_test.ravel()),\n",
    "               \"F1 Score Weighted: \" : f1_score(y_test, y_pred, average='weighted'), \n",
    "               \"F1 Score per Class: \" : fscore,\n",
    "               \"Precision per Class: \" : precision,\n",
    "               \"Recall per Class: \" : recall,\n",
    "               \"Support:  \" : support}\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    disp.plot()\n",
    "    plt.savefig(confusion_matrix_filename, dpi=500)                                                                                     #https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"CLASSES: Clear = 0, Rain = 1, Partially Cloudy = 2, Overcast = 3, Snow = 4\" + \"\\n\")\n",
    "    with open(metrics_filename, 'w') as f:\n",
    "        for k, v in metrics.items():\n",
    "            f.write(str(k) + str(v) + '\\n\\n')\n",
    "            print(str(k) + str(v))\n",
    "    f.close()\n",
    "    \n",
    "def interractive_plot(X, y, a,b,c, interractive):\n",
    "    classes = {'#ef8a62': '0', '#1cfc03': '1', '#67a9cf': '2', '#fc0303' : '3', '#00ffe5' : '4'}\n",
    "    legend = {'#ef8a62': 'Fair', '#1cfc03': 'Rain', '#67a9cf': 'Cloudy', '#fc0303' : 'Overcast', '#00ffe5' : 'Snow'}\n",
    "    colors = dict()\n",
    "    for j, item in enumerate(y):\n",
    "        for k, v in classes.items():\n",
    "            if item == int(v): colors[j] = k\n",
    "    \n",
    "    if interractive is True:\n",
    "        #https://plotly.com/python/3d-scatter-plots/\n",
    "        fig = px.scatter_3d(X, a, b, c, color=colors, width=1300, height=1000,\n",
    "                           labels={\"x\" : \"Temperature\", \"y\" : \"Humidity\", \"z\" : \"Pressure\"}, title=\"CLASSES CLUSTERS\")  \n",
    "        fig.for_each_trace(lambda t: t.update(name = legend[t.name]))\n",
    "        fig.update_layout(legend_title_text='WEATHER')\n",
    "        fig.show()\n",
    "        \n",
    "    else:\n",
    "        #https://www.analyticsvidhya.com/blog/2021/10/interactive-plots-in-python-with-plotly-a-complete-guide/  <= great matplotlib article\n",
    "        fig = plt.figure(figsize = (10, 10))\n",
    "        ax = plt.axes(projection =\"3d\")\n",
    "        kwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\n",
    "        ax.scatter3D(a, b, c, c=colors, **kwarg_params)\n",
    "        ax.set_xlabel('Temperature', fontweight ='bold')\n",
    "        ax.set_ylabel('Humidity', fontweight ='bold')\n",
    "        ax.set_zlabel('Pressure', fontweight ='bold')\n",
    "        plt.suptitle(\"Dataset Resampled with SMOTE\")\n",
    "        plt.show()\n",
    "        \n",
    "def oversampling(X_train, y_train, method):\n",
    "    if method == 1: X_train, y_train = SMOTE().fit_sample(X_train, y_train)\n",
    "    else: X_train, y_train = ADASYN().fit_sample(X_train, y_train)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    print(\"Total training data: \", X_train.size)\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #dataset selection and loading\n",
    "    train_set_name = \"weather_data_2000_2019\"\n",
    "    train_dataset = pd.read_csv((\"./datasets/\" + train_set_name + \".csv\"),header=None)\n",
    "    test_set_name = \"weather_data_2020_2021\"\n",
    "    test_dataset = pd.read_csv((\"./datasets/\" + test_set_name + \".csv\"),header=None) \n",
    "\n",
    "    print(\"Your Train Dataset is: \", train_set_name)                                                                       #display dataset name to user\n",
    "    print(\"Your Test Dataset is: \", test_set_name)                                                                       #display dataset name to user\n",
    "\n",
    "    features_selected = 5\n",
    "    #training features\n",
    "    temperature_train = train_dataset.iloc[:, 4:5]\n",
    "    feels_like_train = train_dataset.iloc[:, 7:8]\n",
    "    dew_point_train = train_dataset.iloc[:, 8:9]\n",
    "    humidity_train = train_dataset.iloc[:, 9:10]\n",
    "    pressure_train = train_dataset.iloc[:, 19:20] \n",
    "    #uv_index_train = train_dataset.iloc[:, 24:25] \n",
    "\n",
    "    #testing data\n",
    "    temperature_test = test_dataset.iloc[:, 4:5]\n",
    "    feels_like_test = test_dataset.iloc[:, 7:8]\n",
    "    dew_point_test = test_dataset.iloc[:, 8:9]\n",
    "    humidity_true = test_dataset.iloc[:, 9:10]\n",
    "    pressure_test = test_dataset.iloc[:, 19:20]\n",
    "    #uv_index_test = test_dataset.iloc[:, 24:25] \n",
    "\n",
    "    X_train = pd.concat([temperature_train, feels_like_train, dew_point_train, humidity_train, pressure_train], axis=1)   #, uv_index_train], axis=1)\n",
    "    X_train.replace(([np.inf, -np.inf], np.nan), inplace=True)                                                            #replace any infinite values with nan\n",
    "    X_train = X_train.to_numpy()  \n",
    "\n",
    "    y_train = train_dataset.iloc[:, 30:31].to_numpy()\n",
    "    y_true = test_dataset.iloc[:, 30:31].to_numpy()\n",
    "\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "    test_data = pd.concat([temperature_test, feels_like_test, dew_point_test, humidity_true, pressure_test], axis=1)       #, uv_index_test], axis=1)       \n",
    "    test_data.replace(([np.inf, -np.inf], np.nan), inplace=True)                                                           #replace any infinite values with nan\n",
    "    X_test = test_data.to_numpy()\n",
    "\n",
    "    #change all nan values in all datasets with the most frequent value of the dataset\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    imp.fit(X_train)\n",
    "    imp.fit(y_train)\n",
    "    imp.fit(X_test)\n",
    "    imp.fit(y_true)\n",
    "    \n",
    "    disp = int(input(\"Please choose 1 to display the dataset or any button to cotinue without displaying!\"))\n",
    "    if disp == 1:\n",
    "        display(X_train)\n",
    "        display(y_train.ravel())\n",
    "\n",
    "        display(X_test)\n",
    "    else: print(\"Not displaying dataset!\")   \n",
    "        \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    rebalance = int(input(\"PLEASE CHOOSE 1 TO REBALANCE DATASET WITH SMOTE. CHOOSE 2 TO REBALANCE DATASET WITH ADASYN\" + \n",
    "                          \"\\nPRESS ANYTHING ELSE TO CONTINUE WITH ORIGINAL!\\n\"))\n",
    "    if rebalance == 1:\n",
    "        X_train, y_train = oversampling(X_train, y_train, 1)\n",
    "    elif rebalance == 2:\n",
    "        X_train, y_train = oversampling(X_train, y_train, 2)\n",
    "    else: print(\"Total training data: \", X_train.size)\n",
    "    \n",
    "    interractive_plot(X_train, y_train, X_train[:, 0], X_train[:, 3], X_train[:, 4], True)\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    \n",
    "    choice = int(input(\"Please choose 1 for Training and 2 for Random CV Search.\" +\n",
    "                       \"\\nPressing 3 will load the optimised Gaussian Naive Bayes Classifier model!\\n\"))\n",
    "    if rebalance == 1: clf_type = \"default_rebalanced_SMOTE\" if choice == 1 else \"optimised_rebalanced_SMOTE\" \n",
    "    elif rebalance == 2: clf_type = \"default_rebalanced_ADASYN\" if choice == 1 else \"optimised_rebalanced_ADASYN\" \n",
    "    else: clf_type = \"default\" if choice == 1 else \"optimised\"\n",
    "    model_name = \"./trained_models/xgb_\" + clf_type + \"_\" + train_set_name + \"_\" + test_set_name + \"_\" + str(features_selected) + \"f.p\"\n",
    "    confusion_matrix_filename = \"./confusion_matrix/xgb_\" + str(clf_type) + \"_\" + train_set_name + \"_\"  + test_set_name + \"_\" + str(features_selected) + \"f.png\"\n",
    "    metrics_filename = \"./metrics/xgb_\" + str(clf_type) + \"_\" + train_set_name + \"_\"  + test_set_name + \"_\" + str(features_selected) + \"f.txt\"\n",
    "    csv_filename = \"./outputs/output_xgb_\"  + str(clf_type) + \"_\" + train_set_name + \"_\"  + test_set_name + \"_\" + str(features_selected) + \"f.csv\"\n",
    "\n",
    "    if choice == 1:\n",
    "        # Create XGBoost classifer object and train it on the selected dataset\n",
    "        xgb = (XGBClassifier(objective = 'multi:softmax', eval_metric = 'merror')).fit(X_train, y_train.ravel())\n",
    "        save_statistics(xgb, X_test, y_true, csv_filename, confusion_matrix_filename, metrics_filename)\n",
    "\n",
    "    elif choice == 2:\n",
    "        #random search: https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
    "        #grid search: https://www.mikulskibartosz.name/xgboost-hyperparameter-tuning-in-python-using-grid-search/\n",
    "        #label encoder warning: https://stackoverflow.com/questions/45251126/deprecation-warning-on-xgboost-sklearn\n",
    "        #https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook\n",
    "        estimator = XGBClassifier(objective = 'multi:softmax', nthread = 20, seed = 42, num_class = 5, eval_metric = 'merror')\n",
    "        parameters = {'max_depth': range(2, 10, 1),'n_estimators': range(60, 220, 40), 'learning_rate': [0.1, 0.01, 0.05]}\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=parameters, cv = 5, verbose = True,  refit = True)\n",
    "        grid_search.fit(X_train, y_train.ravel())\n",
    "        \n",
    "        print(grid_search.best_params_)\n",
    "        print(grid_search.best_estimator_)\n",
    "        pickle.dump(grid_search.best_estimator_, open(model_name, \"wb\"))\n",
    "\n",
    "    elif choice == 3:\n",
    "        xgb = pickle.load(open(model_name, \"rb\"))\n",
    "        print(\"Parameters Classifier Loaded: \", xgb.get_params())\n",
    "\n",
    "        save_statistics(xgb, X_test, y_true, csv_filename, confusion_matrix_filename, metrics_filename)\n",
    "\n",
    "if __name__ == \"__main__\":                                                                                               #guard boilerplate\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
